# (딥러닝)데이터 전처리, 규제, 교차검증

### **데이터 전처리**

데이터 전처리는 기존의 데이터를 머신러닝 알고리즘에 알맞은 데이터로 변환하는 과정이다.

전처리 과정은 모델이 생성 된 이후에도 예측하고자 하는 새로운 데이터에도 적용하는 과정이다. 

또한 전처리 과정을 통해서 더욱더 모델 학습의 성능을 높일 수 있다.

- 데이터 전처리 과정
    1. Data cleaning - 대부분의 머신러닝 알고리즘은 누락된 데이터(결측치)가 있을 때, 정상적으로 작동X
        - 결측치 처리(missing value) - 결측치가 포함되어 있는 항목을 모두 버리거나, 적절한 값으로 대체하거나, 아니면 결측치를 NA(not available)라고 표시하여 다음의 분석단계로 결측치 처리를 넘김.
        - 틀린(invalid) 데이터 처리 - 일반적으로 결측치 처리 방법과 같음.
        - 이상치 처리 - 일반적인 범위를 벗어나는 값(이상치)를 찾는 것.
    2. Handling text and categorical attribute - 데이터 세트에 있는 텍스트가 있는 경우 숫자형으로 변환 해준다.
        - factorize(), OrdinalEncoder(): 숫자형 또는 범주형으로 변환해주는 함수. ※factorize()-pandas, OrdinalEncoder()-Scikit-learn
    3. Custom Transformers - 데이터를 분석하기 좋은 형태로 바꾸는 작업, Scikit-learn에서 제공하는 다양한 데이터 변환기를 이용하여 커스텀 변환기를 만들수 있다. 
        - fit(), transform(),  fit_transform()
        - 다양한 변환법 : 로그변환, 역수 변환, 데이터 축소 ...
    4. Feature scaling - 일반적인 머신러닝 알고리즘은 아주 다양한 범위의 데이터를 학습시키면 성능이 좋지 못하기 때문에 숫자형 데이터의 범위를 줄여주어야한다. 또한 숫자형 데이터로 이루어진여러 개의 feature를 사용할 때 feature간의 범위가 다를 때에도 같은 범위로 변환해주는 일반정규화 과정을 거쳐야한다.
    5. Transformation pipelines - 아래와 같은 일련의 과정을 자동화한 함수를 Machine Learning Pipeline이라고 한다.
        - load data
        - Data anlaysis
        - Data processing
        - Data split & validation
        - Build & Train model
        - model validation

### 훈련용 데이터와 테스트 데이터

데이터 분석에선 모델을 만드는 과정과 모델을 검증하는 과정, 두 단계의 절차가 필요하다.

- 모델을 만드는 과정을 훈련(**training**)이라고 함.이때 사용되는 데이터를 훈련용 데이터(**training data**)라고 함. 훈련 과정은 각 모델을 구성하는 파라미터를 찾는 과정이라고 할 수 있음.
- 모델의 성능을 검증하는 것을 test라고 하고 이 때 사용되는 데이터를 테스트 데이터(test data)라고 함.
- 훈련용, 테스트용 데이터를 준비할 때 랜덤한 성질을 보장하는 것이 필요함. 훈련용 데이터와 테스트 데이터는 보통 7:3 ~ 8:2 비율로 나누도록 한다.

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()
x = cancer.data
y = cancer.target
x_train_all, x_test, y_train_all, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42)

from sklearn.linear_model import SGDClassifier   
sgd = SGDClassifier(loss='log', random_state=42) # 하이퍼파라미터를 로지스틱 손실함수로 사용
sgd.fit(x_train_all, y_train_all)
sgd.score(x_test, y_test)

from sklearn.linear_model import SGDClassifier
sgd = SGDClassifier(loss='hinge', random_state=42) # 하이퍼파라미터를 hinge 손실함수로 사용
sgd.fit(x_train_all, y_train_all)                  # hinge 손실함수 -> 서포트 벡터 머신(SVM)
sgd.score(x_test, y_test)
```

train set를 이용해 만들어진 모델을 다양한 머신러닝 최적화 기술을 이용해 튜닝하여 올바른 일반화 성능을 구해야한다. ※여기서 일반화 성능은 학습에 사용되지 않은 데이터(Test set)를 이용해 측정된 모델의 성능을 뜻한다. 

위 코드와 같이 테스트 세트를 이용해 모델을 튜닝하면 올바른 일반화 성능을 추정하기는 힘들다. test set의 한 가지 구성으로만 튜닝이 되기 때문이다.(test set에만 적합한 모델이 구축되기 때문) test set와 다른 구성을 가진 data(실전데이터)를 모델에 적용했을 때 그 성능에 큰 차이가 생길 수 있다.

이를 해결하기 위해 **검증세트**를 준비한다.

### 검증세트(개발세트)

<img src="./(딥러닝)데이터 전처리, 규제, 교차검증/Untitled.png" width="50%" height="50%" />

훈련세트를 훈련세트와 검증세트 두가지로 한 번 더 분류한다. 이 검증세트를 이용해 하이퍼파라미터 튜닝하고 테스트 세트는 학습이 완료된 최적의 모델의 일반화 성능을 마지막으로 추정할 때 사용한다.

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()
x = cancer.data
y = cancer.target
x_train_all, x_test, y_train_all, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42)

#검증 세트 분류
x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)
print(len(x_train), len(x_val))
```

<img src="./(딥러닝)데이터 전처리, 규제, 교차검증/Untitled 1.png" width="50%" height="50%" />

<img src="./(딥러닝)데이터 전처리, 규제, 교차검증/Untitled 2.png" width="50%" height="50%" />

데이터의 범위가 크면 최적의 값을 찾아가는데 훈련 속도나 횟수가 낭비된다. 따라서 표준화를 통해 데이터의 범위를 줄여서 모델을 훈련해야 한다.

### 검증세트로 모델 성능 평가

```python
import numpy as np
train_mean = np.mean(x_train, axis=0) # 훈련용 데이터 평균
train_std = np.std(x_train, axis=0)   # 훈련용 데이터 표준편차
x_train_scaled = (x_train - train_mean) / train_std # 훈련용 데이터 표준화

layer2 = SingleLayer() 
layer2.fit(x_train_scaled, y_train) # 표준화한 데이터로 훈련

val_mean = np.mean(x_val, axis=0) # 검증세트 평균
val_std = np.std(x_val, axis=0)   # 검증세트 표준편차
x_val_scaled = (x_val - val_mean) / val_std # 검증세트 표준화
layer2.score(x_val_scaled, y_val)
```

### 올바르게 검증 세트 전처리

```python
import matplotlib.pyplot as plt
x_val_scaled = (x_val - train_mean) / train_std # 훈련용 데이터 평균과 표준편차로 표준화해야 함
plt.plot(x_train_scaled[:50,0], x_train_scaled[:50,1], 'co')
plt.plot(x_val_scaled[:50,0], x_val_scaled[:50,1], 'yo')
plt.xlabel('fature 1')
plt.ylabel('fature 2')
plt.legend(['train set', 'val.set'])
plt.show()
```

<img src="./(딥러닝)데이터 전처리, 규제, 교차검증/Untitled 3.png" width="50%" height="50%" />

원본 데이터의 분포도

<img src="./(딥러닝)데이터 전처리, 규제, 교차검증/Untitled 4.png" width="50%" height="50%" />

올바르게 검증 세트 전처리한 후 분포도

---

### 과대적합과 과소적합

<img src="./(딥러닝)데이터 전처리, 규제, 교차검증/Untitled 5.png" width="50%" height="50%" />

- **과대적합(overfitting)**

    모델이 훈련 데이터에 너무 잘 맞지만 일반성이 떨어진다는 의미. 즉, 모델이 훈련 세트에 너무 잘 맞아 훈련세트 이외의 다양한 변수(테스트, 검증세트를 포함한)에 대응하기 힘들다. 모델의 복잡도가 필요 이상으로 높을 때에도 발생한다. 훈련 데이터 세트에 잡음이 너무 많거나 데이터자체가 너무 적으면 잡음까지 감지하여 모델이 학습될 수 있어 주의해야 한다.

<해결방법>

- 훈련 세트의 크기를 키운다.
- 정규화, 규제 등 다양한 방법을 이용해 적당한 복잡도를  가진 모댈을 찾아준다. >모델의 복잡도를 낮춘다.
- 훈련 세트의 잡음을 줄인다(오류, 이상치제거)

- **과소적합(underfitting)**

    과대적합의 반대. 모델이 너무 단순하여 훈련 데이터의 구조를 올바르게 학습하지 못 했을 때 발생한다. 검증세트의 정확도와 훈련세트의 정확도가 비슷하지만 모델의 성능은 낮다.

<해결방법>

- 모델의 복잡도를 높힌다.
- 모델의 제약을 줄인다. >규제 하리퍼파라미터 값 줄이기
- 조기종료 시점까지 충분히 학습시킨다. >에포크값을 높힌다.

모델의 복잡도가 높아지는 것을 방지(모델을 단순하게)하기 위해 모델에 제약을 거는 것을 규제라 한다. 규제의 양을 결정하는 매개변수가 **하이퍼파라미터**이다. 규제 하이퍼파라미터를 큰값으로 지정할수록 복잡도가 낮은 단순한 모델을 얻게 된다.

<img src="./(딥러닝)데이터 전처리, 규제, 교차검증/Untitled 6.png" width="50%" height="50%" />

에포크와 손실함수로 표현한 과대과소접합그래프이다. 에포크의 수가 높아질수록 즉, 훈련의 수가 많아질수록 검증세트의 손실이 높아지고 정확도는 떨어진다.(훈련이 거듭될수록 overfitting되기 때문) 따라서 검증세트의 손실이 가장 작고, 정확도가 가장 높은 지점(최적점)에서의 에포크를 조기종료 지점으로 설정해야한다.

<img src="./(딥러닝)데이터 전처리, 규제, 교차검증/Untitled 7.png" width="50%" height="50%" />

모델의 복잡도가 높아질수록 모델은 overfitting되고 낮아질수록 underfitting된다. 에포크로 표현한 그래프와 마찬가지로 모델의 손실이 가장 낮고 정확도가 가장 높은 지점(최적점)을 찾아 모델에 가장 적절한 복잡도를 설정해야한다.

적절한 편향-분산 트레이드오프를 선택해야한다~

```python
layer4 = SingleLayer()
layer4.fit(x_train_scaled, y_train, epochs=20) # epochs 20에서 조기종료
layer4.score(x_val_scaled, y_val)
```

### 규제

overfitting이 일어나는 경우는 크게 **1) 데이터가 부족할 때** **2) 모델이 너무 복잡할 때**의 두 가지 경우로 나눌 수 있다. 즉, 모델의 복잡도에 걸맞는 데이터가 주어지지 않았을 때 과대적합이 발생하는 것이다. 모델이 복잡하다면 그만큼 많은 데이터가 필요한데 한정된 데이터 안에서도 과대적합되지 않고 좋은 성능의 예측 모델을 만들기위해서 모델의 복잡도를 낮추도록 한다. 복잡도를 낮추는 방법 중의 하나인 규제를 알아보자.

규제란, 가중치 갱신시, 훈련 데이터에 특화된 가중치에 어떠한 규제 값을 적용하여 오버피팅을 예방하는 방법이다. 

가중치는 손실함수로 인하여 수정된다. 이 과정에서 데이터가 부족하면 훈련세트에 최적화된 모델(overfitting된 모델)이 만들어 질 수 있는데, 여기서 손실함수의 값에 규제 값을 적용하여여 가중치의 업데이트를 둔화시켜 모델의 과대적합을 막도록 한다.

즉, **규제된 손실함수의 값 = 손실함수 + 규제값** 이라고 할 수 있는데 여기 **'규제값'**에 어떤 것을 넣느냐에 따라 L1, L2 규제로 나뉜다.

**norm(노름)** : 백터의 길이나 크기를 알아내기 위한 방식으로 손실함수에 적용되는 값. L1 norm은 벡터 내의 요소(w)의 절대값, L2 norm은 백터 내 요소(w)의 제곱에 루트를 씌운 값이다. 각 norm의 w가 최종 결과에 미치는 손실함수의 기울기(미분값)에 영향을 주기 위해 손실함수에 적용한다.

- **L1 규제 (손실함수 + L1 norm)**

    라쏘(Lasso)라고도 불린다. 수식으로 표현하면

    **L1 = loss + α * sum(|wi|)**

    으로 표현된다. 

    ※ L1 노름 특성상 기울기 소실(기울기 0)이 발생되어 해당하는 가중치가 비활성화되기도 한다.

- **L2 규제 (손실함수 + L2 norm)**

    릿지(Ridge)라고도 불린다. 수식으로 표현하면

    **L2 = loss + α *root(sum(wi^2))**

    라고 표현된다.

    ※L2 노름의 특성상 가중치가 완전히 0이 되진 않는다.

기존 손실함수에 L1, L2 norm에 알파값을 곱해준 것을 더해주는 것이다. α(알파)는 우리가 설정하는 하이퍼파라미터 값이다. α를 0으로 설정하면 L1, L2 노름은 무의미해진다. 따라서 **알파가 0에 가까울수록 규제가 약해지고 커질수록 규제가 강해진다**. 

### 교차검증

<img src="./(딥러닝)데이터 전처리, 규제, 교차검증/Untitled 8.png" width="50%" height="50%" />

### 파이프라인

검증 세트를 따로 나누지 않고 파이프라인을 사용해 전처리 단계를 포함해 교차 검증을 수행할 수 있다.

```python
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import numpy as np
pipe = make_pipeline(StandardScaler(),sgd)
scores = cross_validate(pipe, x_train_all, y_train_all, cv=10, return_train_score=True)
print(np.mean(scores['test_score']))
```

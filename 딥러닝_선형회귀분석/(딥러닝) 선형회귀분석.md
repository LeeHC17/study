# (딥러닝) 선형회귀분석

### 회귀분석이란?

독립변수(x,data)와 종속변수(y,target) 사이에 관계가 존재할 때, 두 변수 사이의 모형(모델)을 구하고 관계에 유의미성, 정도를 분석하는 방법.

### 선형회귀란?

![(%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC)%20%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20d492a4941cd345b1934d951db701d6c4/Untitled.png]((%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC)%20%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20d492a4941cd345b1934d951db701d6c4/Untitled.png)

x좌표는 샘플의 특성, y좌표는 라벨, 결과값을 뜻함

선형적으로 분포되어 있는 데이터를 가장 잘 표현하는(대표하는) 하나의 선을 찾는 분석기법. 즉, 독립변수 x와 종속변수 y의 선형관계를 모델링 하는 것.

- 회귀선: 데이터를 대표하는 하나의 선.
- 회귀식: 회귀선을 함수로 표현한 식.

     $ŷ = w*x + b$     ※ ŷ: 예측값  x: 입력, 특성  w: 가중치, 회귀계수  b: 절편, 편향

선형회귀분석을 통해 데이터(여러개의 샘플의 특성값, 결과값)를 이용해 회귀식에서 최적의 w, b를 구해야한다. 구해진 가중치와 절편으로 새로운 데이터의 특성값으로 부터 예측값을 찾도록 한다.

### 경사하강법이란?

![(%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC)%20%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20d492a4941cd345b1934d951db701d6c4/Untitled%201.png]((%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC)%20%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20d492a4941cd345b1934d951db701d6c4/Untitled%201.png)

선형회귀분석의 w(가중치)와 b(절편)을 찾는 기법 중 하나.

w, b의 손실함수의 기울기를 구하고 기울기의 절대 값이 낮은 쪽으로 값을 변형시켜 함수의 기울기가 최소가 되는(**기울기가 0**이 되는) 회귀식을 찾는 방법.

<경사하강법 순서>

1. 무작위로 w와 b를 결정. (무작위 모델 생성)
2. x에서 샘플 하나 선택, ŷ 계산. (무작위 모델 예측)
3. ŷ, 진짜 y 비교
4. ŷ과 y가 가까워지도록 w, b 조정
5. 반복

**3. ŷ, 진짜 y 비교**

y(타깃)와 ŷ(예측)의 오차를 구한다.

$y - ŷ = err$

**4. ŷ과 y가 가까워지도록 w, b 조정**

가중치와 절편을 업데이트한다. 

- **가중치 : w_new**

$변화율 = \frac{(ŷ\_i- ŷ)}{(w\_i- w)} = \frac{(x[0] * w\_i+ b) - (x[0] * w + b)}{w\_i- w} = \frac{x[0] * ((w + 0.1) - w)}{(w + 0.1) - w} = x[0]$

가중치의 변화율을 계산하면 x가 나오는 것을 알 수 있다.  가중치를 업데이트 하기 위해 현재 w에 변화율을 더한다. 변화율이 양수라면 예측값이 증가하고, 음수라면 예측값이 감소한다.

- **절편 : b_new**

$변화율 = \frac{(ŷ\_i- ŷ)}{(b\_i-b)} = \frac{(x[0]*w+b\_i)-(x[0]*w+b)}{b\_i-b} = \frac{(b+0.1) - b}{(b+0.1)-b} = 1$

> 가중치와 절편을 각각의 변화율을 더하여 업데이트 하는 방법은 변화율에 비해 ŷ과 y의 차이가 클 때, ŷ이 y보다 커졌을 때 사용하기 부적합하다. 따라서 ŷ과 y의 차이에 능동적으로 대처될 수 있어야 한다.

>> **오차 역전파로 가중치와 절편을 업데이트** 하는 방법 사용

$w\_new = w + w\_rate * err$

$b\_new = b + 1 * err$

### 손실함수(비용함수)

w, b을 임의로 설정한 회귀식과 데이터 사이의 결과값의 평균제곱오차(손실함수)를 구한다. 
# 여기서 제곱오차를 사용하는 이유는 실제값과 예측값의 차이를 더 극명하게 하기 위함이라 함

$MSE = \frac{1}{n}\sum\limits_{i=1}^n(y_i-ŷ_i)^2$

$SE = (y-ŷ)^2$

![(%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC)%20%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20d492a4941cd345b1934d951db701d6c4/Untitled%202.png]((%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC)%20%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20d492a4941cd345b1934d951db701d6c4/Untitled%202.png)

w와 b에 대한 손실함수를 그래프로 표현한 것

![(%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC)%20%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20d492a4941cd345b1934d951db701d6c4/Untitled%203.png]((%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC)%20%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20d492a4941cd345b1934d951db701d6c4/Untitled%203.png)

w와 b의 기울기가 최소가 될 때

y와 ŷ의 차이를 최소화시키는 w, b를 찾아야 한다. 두 값의 손실함수의 접선의 기울기의 절대값이 0이 될 때가 최적의 w, b값이 된다.

- w 손실함수의 기울기를 찾기 위해 미분한다.

    $\frac{dSE}{dw} = \frac{d}{dw}(y - ŷ)(-\frac{d}{dw}ŷ) = 2(y - ŷ)(-x) = -2(y-ŷ)x$ 

    이 때 SE = $\frac{1}{2}(y-ŷ)^2$ 라면  $\frac{dSE}{dw} = -(y-ŷ)x$

    - 미분된 결과 $\frac{dSE}{dw} = -(y-ŷ)x$ 를 가중치에서 빼면 가중치의 손실함수의 낮은 쪽으로 접선이 이동한다.

$w = w - \frac{dSE}{dw} = w + (y-ŷ)x$ 

-w에 대한 제곱오차 미분값, 그레디언트(gradient)

$w\_new = w + w\_rate * err$

-오차역전파

- b 손실함수의 기울기 찾기 위해 미분한다.

    $\frac{dSE}{db} = \frac{d}{db}\frac{1}{2}(y-ŷ)^2 = (y-ŷ)(-\frac{d}{db}ŷ) = (y-ŷ)(-1)=-(y-ŷ)1$

    - 마찬가지로 미분된 결과를 절편에서 빼면 손실함수의 낮은쪽으로 접선이 이동한다.

$b = b-\frac{dSE}{db}=b+(y-ŷ)$

-b에 대한 제곱오차 미분값, 그레디언트(gradient)

$b=b+1*err$

-오차역전파

>직관적으로 계산한 오차 역전파와 가중치를 업데이트한 **결과가 같다.**

업데이트 과정을 반복하면 최적의 w, b 값을 찾을 수 있다.